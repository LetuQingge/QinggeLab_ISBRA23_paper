{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6183bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: color: not found\n",
      "2023-06-08 12:56:15.613178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 12:56:15.717463: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-08 12:56:16.266009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/rkannan/miniconda3/envs/richard_tf/lib/\n",
      "2023-06-08 12:56:16.266071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/rkannan/miniconda3/envs/richard_tf/lib/\n",
      "2023-06-08 12:56:16.266076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "sh: 1: color: not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('color')\n",
    "import random\n",
    "from termcolor import colored\n",
    "from typing import List, Optional\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import utility as ut\n",
    "\n",
    "os.system('color')\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DATAPATH = os.path.join(os.getcwd(), \"newdatasets\")\n",
    "RESULTS_PATH = os.path.join(os.getcwd(), \"autoencoder_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a55d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "class PaddingType(Enum):\n",
    "    ZERO=1\n",
    "    TRUNCATE=2\n",
    "    EMPTY=3\n",
    "\n",
    "# =============================================================================\n",
    "class NoisificationMethod(Enum):\n",
    "    RANDOMSCATTER=1\n",
    "    RANDOMCONTIG=2\n",
    "# =============================================================================\n",
    "def highlight_indices(seq: np.array, indices: np.array, color: str):\n",
    "    # We use deepcopy to prevent mutation and we cast to object so that we can treat contents as python strings\n",
    "    # otherwise, it gets messed up as it treats each element as a single character\n",
    "    newseq = deepcopy(seq).astype('object')\n",
    "    newseq[indices] = np.vectorize(lambda x: colored(x, color, attrs=[\"bold\"]))(seq[indices])\n",
    "    return newseq\n",
    "\n",
    "# =============================================================================\n",
    "def print_sequence(seq, \n",
    "                   header: str=None, \n",
    "                   incorrect_indices: Optional[np.array]=None, \n",
    "                   correct_indices: Optional[np.array]=None):\n",
    "\n",
    "    newseq = deepcopy(seq)\n",
    "    if correct_indices is not None and correct_indices.size != 0:\n",
    "        newseq = highlight_indices(newseq, correct_indices, \"green\")\n",
    "    if incorrect_indices is not None and incorrect_indices.size != 0:\n",
    "        newseq = highlight_indices(newseq, incorrect_indices, \"red\")\n",
    "\n",
    "    line_length = 40\n",
    "    if header:\n",
    "        print(header)\n",
    "    print(\"=\" * line_length)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(newseq):\n",
    "        print(\" \".join(newseq[i: i+line_length]))\n",
    "        i += line_length\n",
    "\n",
    "# =============================================================================\n",
    "def get_sequences(fasta_file: str) -> List[np.array]:\n",
    "    sequences = []\n",
    "    lines = []\n",
    "    with open(fasta_file, \"r\") as input_file:\n",
    "        lines = list(filter(None, input_file.read().split(\"\\n\")))\n",
    "\n",
    "    parts = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if parts:\n",
    "                sequences.append(np.array([c for c in \"\".join(parts)]))\n",
    "            parts = []\n",
    "        else:\n",
    "            parts.append(line)\n",
    "    if parts:\n",
    "        sequences.append(np.array([c for c in \"\".join(parts)]))\n",
    "    return sequences\n",
    "\n",
    "# =============================================================================\n",
    "def snake_case_prettify(s):\n",
    "    return \" \".join(w.capitalize() for w in s.split(\"_\"))\n",
    "\n",
    "# =============================================================================\n",
    "def save_models(models):\n",
    "    modeldir = os.path.join(os.getcwd(), \"1Dmodels\")\n",
    "    if not os.path.exists(modeldir):\n",
    "        os.makedirs(modeldir)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.save(os.path.join(modeldir, name))\n",
    "\n",
    "# =============================================================================\n",
    "def load_models():\n",
    "    modeldir = os.path.join(os.getcwd(), \"1Dmodels\")\n",
    "    if os.path.exists(modeldir):\n",
    "        return {f: tf.keras.models.load_model(os.path.join(modeldir, f)) for f in os.listdir(modeldir)}\n",
    "    return {}\n",
    "\n",
    "# =============================================================================\n",
    "def print_diff_between_target_and_de_novo():\n",
    "\n",
    "    dataset_dir = os.path.join(os.getcwd(), \"newdatasets\")\n",
    "    \n",
    "    de_novo_sequence = get_sequences(os.path.join(dataset_dir, \"denovo_0.20_6.txt\"))[0]\n",
    "    target_sequence = get_sequences(os.path.join(dataset_dir, \"target_sequence.txt\"))[0]\n",
    "\n",
    "    gap_indices = np.where(de_novo_sequence == '-')[0]\n",
    "    incorrect_indices = np.where(de_novo_sequence != target_sequence)[0]\n",
    "    correct_indices = np.where(de_novo_sequence == target_sequence)[0]\n",
    "    incorrect_non_gaps = np.setdiff1d(incorrect_indices, gap_indices)\n",
    "    print(f\"Length of target: {len(target_sequence)}\")\n",
    "    print(f\"Number of incorrect non-gaps: {len(incorrect_non_gaps)}\")\n",
    "    print(f\"Number of gaps: {len(gap_indices)}\")\n",
    "\n",
    "    print_sequence(de_novo_sequence, \"Protein Scaffold\", incorrect_indices, correct_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea490d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_hp_build(hp):\n",
    "    \n",
    "    conv1_filter_size_ = hp.Int(\"conv1_filter_size\",min_value=4, max_value=10,step = 1)\n",
    "    conv2_filter_size_ = hp.Int(\"conv2_filter_size\",min_value=4, max_value=10,step = 1)\n",
    "    conv1_filters_ = hp.Int(\"conv1_filters\",min_value=30, max_value=50,step = 4)\n",
    "    conv2_filters_ = hp.Int(\"conv2_filters\",min_value=50, max_value=100,step = 4)\n",
    "    bridge_filters_ = hp.Int(\"bridge_filters\",min_value=120, max_value=200,step = 8)\n",
    "    bridge_filter_size_ = hp.Int(\"bridge_filter_size\",min_value=4, max_value=10,step = 1)\n",
    "    max_pool_ = hp.Int(\"max_pool\",min_value=2, max_value=4,step = 1)\n",
    "    dropout_ = hp.Choice(\"dropout_: \",[0.25, 0.35, 0.40,0.50])\n",
    "    # =============================================================================\n",
    "    class Encoder(layers.Layer):\n",
    "        \"\"\"Encoder part of autoencoder\"\"\"\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def __init__(self, conv1_filters, conv2_filters, conv1_filter_size=conv1_filter_size_, conv2_filter_size=conv2_filter_size_, maxpool=2, dropout=0.25, name=\"encoder\", **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "\n",
    "            self.conv1 = layers.Conv1D(conv1_filters, conv1_filter_size, padding=\"same\", activation=\"relu\")\n",
    "            self.conv2 = layers.Conv1D(conv2_filters, conv2_filter_size, padding=\"same\", activation=\"relu\")\n",
    "            self.maxpool = layers.MaxPooling1D(maxpool, padding=\"same\")\n",
    "            self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def call(self, inputs):\n",
    "            return self.dropout(self.maxpool(self.conv2(self.dropout(self.maxpool(self.conv1(inputs))))))\n",
    "\n",
    "    # =============================================================================\n",
    "    class Decoder(layers.Layer):\n",
    "        \"\"\"Decoder part of autoencoder\"\"\"\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def __init__(self, conv1_filters, conv2_filters, conv1_filter_size=conv1_filter_size_, conv2_filter_size=conv2_filter_size_, maxpool=2, dropout=0.25, name=\"decoder\", **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "\n",
    "            self.conv1 = layers.Conv1DTranspose(conv1_filters, conv1_filter_size, padding=\"same\", activation=\"relu\")\n",
    "            self.conv2 = layers.Conv1DTranspose(conv2_filters, conv2_filter_size, padding=\"same\", activation=\"relu\")\n",
    "            self.upsample = layers.UpSampling1D(maxpool)\n",
    "            self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def call(self, inputs):\n",
    "            return self.dropout(self.upsample(self.conv2(self.dropout(self.upsample(self.conv1(inputs))))))\n",
    "\n",
    "    # =============================================================================\n",
    "    class Autoencoder(Model):\n",
    "        \"\"\"Autoencoder\"\"\"\n",
    "        # -------------------------------------------------------------------------\n",
    "        def __init__(self, num_classes, name=\"autoencoder\", **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "\n",
    "            self.num_classes = num_classes\n",
    "            self.hyperparameters = {\n",
    "                \"conv1_filters\": conv1_filters_,#32,\n",
    "                \"conv2_filters\": conv2_filters_,#64,\n",
    "                \"conv1_filter_size\": conv1_filter_size_,#5,\n",
    "                \"conv2_filter_size\": conv2_filter_size_,#5,\n",
    "                \"bridge_filters\": bridge_filters_,#128,\n",
    "                \"bridge_filter_size\": bridge_filter_size_,#5,\n",
    "                \"dropout\": dropout_,#0.25,\n",
    "                \"maxpool\": max_pool_#2,\n",
    "            }\n",
    "\n",
    "            hp = self.hyperparameters\n",
    "\n",
    "            self.encoder = Encoder(hp[\"conv1_filters\"], hp[\"conv2_filters\"])\n",
    "            self.bridge = layers.Conv1D(hp[\"bridge_filters\"], hp[\"bridge_filter_size\"], padding=\"same\", activation=\"relu\")\n",
    "            self.decoder = Decoder(hp[\"conv2_filters\"], hp[\"conv1_filters\"])\n",
    "            self.finallayer = layers.Conv1D(self.num_classes, hp[\"conv1_filter_size\"], padding=\"same\", activation=\"softmax\")\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def call(self, inputs):\n",
    "            return self.finallayer(self.decoder(self.bridge(self.encoder(inputs))))\n",
    "    \n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "\n",
    "    class Transformer(tf.keras.Model):\n",
    "        def __init__(self, input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "            super(Transformer, self).__init__()\n",
    "            self.input_shape = input_shape\n",
    "            self.head_size = head_size\n",
    "            self.num_heads = num_heads\n",
    "            self.ff_dim = ff_dim\n",
    "            self.num_transformer_blocks = num_transformer_blocks\n",
    "            self.mlp_units = mlp_units\n",
    "            self.dropout = dropout\n",
    "            self.mlp_dropout = mlp_dropout\n",
    "\n",
    "        def transformer_encoder(self, inputs):\n",
    "            # Normalization and Attention\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            x = layers.MultiHeadAttention(\n",
    "                key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout\n",
    "            )(x, x)\n",
    "            x = layers.Dropout(self.dropout)(x)\n",
    "            res = x + inputs\n",
    "\n",
    "            # Feed Forward Part\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "            x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.dropout)(x)\n",
    "            x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "            return x + res\n",
    "\n",
    "        def build_model(self):\n",
    "            inputs = tf.keras.Input(shape=self.input_shape)\n",
    "            x = inputs\n",
    "            for _ in range(self.num_transformer_blocks):\n",
    "                x = self.transformer_encoder(x)\n",
    "\n",
    "            x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "            for dim in self.mlp_units:\n",
    "                x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "                x = layers.Dropout(self.mlp_dropout)(x)\n",
    "            outputs = layers.Dense(np.prod(self.input_shape), activation=\"softmax\")(x)\n",
    "            outputs = layers.Reshape(target_shape=self.input_shape, name='out_recon')(outputs)\n",
    "            return models.Model(inputs, outputs)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = inputs\n",
    "            for _ in range(self.num_transformer_blocks):\n",
    "                x = self.transformer_encoder(x)\n",
    "\n",
    "            x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "            for dim in self.mlp_units:\n",
    "                x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "                x = layers.Dropout(self.mlp_dropout)(x)\n",
    "            outputs = layers.Dense(np.prod(self.input_shape), activation=\"softmax\")(x)\n",
    "            outputs = layers.Reshape(target_shape=self.input_shape, name='out_recon')(outputs)\n",
    "            return outputs\n",
    "        \n",
    "    # =============================================================================\n",
    "    class ProteinScaffoldFixer():\n",
    "        \"\"\"Class to correct errors in a protein scaffold and fill gaps\"\"\"\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def __init__(self, \n",
    "                    output_seqs, \n",
    "                    paddingtype=PaddingType.EMPTY, \n",
    "                    noise_percent=0.2, \n",
    "                    noisemethod=NoisificationMethod.RANDOMSCATTER, \n",
    "                    numgaps=5, \n",
    "                    mingapsize=3, \n",
    "                    mincontigsize=1, \n",
    "                    epochs=200,\n",
    "                    optimizer=\"adam\",\n",
    "                    early_stopping=False):\n",
    "\n",
    "            self.epochs = epochs\n",
    "            self.optimizer = optimizer\n",
    "            self.paddingtype = paddingtype\n",
    "            self.noise_percent = noise_percent\n",
    "            self.early_stopping = early_stopping\n",
    "\n",
    "            self.classes = np.array([\"-\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "            self.ohe = OneHotEncoder(sparse_output=False, categories=[self.classes])\n",
    "\n",
    "            self.max_seq_length = max((len(seq) for seq in output_seqs))\n",
    "            self.maxpool = 2\n",
    "            while self.max_seq_length % (self.maxpool * self.maxpool) != 0:\n",
    "                self.max_seq_length += 1\n",
    "\n",
    "            self.output_seqs = output_seqs\n",
    "            self.input_seqs = self.noisify_sequences(output_seqs, noise_percent, noisemethod, numgaps, mingapsize, mincontigsize)\n",
    "\n",
    "            self.train_x = self.preprocess_sequences(self.input_seqs)\n",
    "            self.train_y = self.preprocess_sequences(self.output_seqs)\n",
    "\n",
    "            self.history = None\n",
    "\n",
    "            self.autoencoder = Autoencoder(len(self.classes))\n",
    "\n",
    "        # =============================================================================\n",
    "        def noisify_sequences(self, seqs, noise_percent, noisemethod, numgaps=5, mingapsize=3, mincontigsize=3):\n",
    "            if noisemethod == NoisificationMethod.RANDOMSCATTER:\n",
    "                return self.noisify_by_random_scatter(seqs, noise_percent)\n",
    "            elif noisemethod == NoisificationMethod.RANDOMCONTIG:\n",
    "                return self.noisify_by_random_contigs(seqs, noise_percent, numgaps, mingapsize, mincontigsize)\n",
    "\n",
    "        # =============================================================================\n",
    "        def noisify_by_random_scatter(self, seqs, noise_percent):\n",
    "            sequences = deepcopy(seqs)\n",
    "            # To noisify our input data, we will replace random amino acids with something else\n",
    "            for seq in sequences:\n",
    "\n",
    "                # We randomly sample from all the possible indices of seq\n",
    "                indices_to_replace = random.sample(range(len(seq)), int(noise_percent * len(seq)))\n",
    "                seq[indices_to_replace] = \"-\"\n",
    "\n",
    "            return sequences\n",
    "\n",
    "        # =============================================================================\n",
    "        def noisify_by_random_contigs(self, seqs, noise_percent, numgaps, mingapsize, mincontigsize):\n",
    "\n",
    "            sequences = deepcopy(seqs)\n",
    "            for seqind, seq in enumerate(sequences):\n",
    "\n",
    "                amino_acid_length = len(seq)\n",
    "                amino_acids_to_replace = int(amino_acid_length * noise_percent)\n",
    "\n",
    "                # The idea is to build a gap queue, randomly putting each amino acid to replace into each gap\n",
    "                # Then, we build a contig queue, randomly putting each amino acid into each contig\n",
    "                gap_queue = np.zeros(numgaps).astype(int)\n",
    "                for _ in range(amino_acids_to_replace):\n",
    "\n",
    "                    # Any gap can be considered\n",
    "                    valid_gaps = np.arange(numgaps)\n",
    "\n",
    "                    # ... so long as we have no underfilled gaps\n",
    "                    underfilled = gap_queue < mingapsize\n",
    "                    # If we have any underfilled, then consider only those until they are no longer underfilled\n",
    "                    if np.any(underfilled):\n",
    "                        valid_gaps = valid_gaps[np.where(underfilled)]\n",
    "\n",
    "                    # Once valid gap indices have been determined, randomly pick one to increment\n",
    "                    gap_queue[np.random.choice(valid_gaps)] += 1\n",
    "\n",
    "                # There can always be one more contig than gaps (if there's a contig at beginnning and end of sequence)\n",
    "                contig_queue = np.zeros(numgaps+1).astype(int)\n",
    "\n",
    "                # We have to allocate all the amino acids NOT in gaps into contigs BETWEEN the gaps\n",
    "                for _ in range(amino_acid_length - amino_acids_to_replace):\n",
    "\n",
    "                    # Any contig can be considered\n",
    "                    valid_contigs = np.arange(len(contig_queue))\n",
    "\n",
    "                    # ... so long as we have no underfilled contigs\n",
    "                    underfilled = contig_queue < mincontigsize\n",
    "\n",
    "                    # The exceptions are the first and last contigs. They are never considered underfilled\n",
    "                    underfilled[0] = False\n",
    "                    underfilled[-1] = False\n",
    "\n",
    "                    # If we have any underfilled, then consider only those until they are no longer underfilled\n",
    "                    if np.any(underfilled):\n",
    "                        valid_contigs = valid_contigs[np.where(underfilled)]\n",
    "\n",
    "                    # Once valid gap indices have been determined, randomly pick one to increment\n",
    "                    contig_queue[np.random.choice(valid_contigs)] += 1\n",
    "\n",
    "                # Once we have determined gap_queue and contig_queue, we iterate over them to set the gaps equal\n",
    "                # to our blank amino acid character\n",
    "\n",
    "                sequence_pointer = 0\n",
    "                iscontig=True\n",
    "                while sequence_pointer < len(seq):\n",
    "                    if iscontig:\n",
    "                        # Don't do anything for the contig except increment the pointer and pop off the contig queue\n",
    "                        sequence_pointer += contig_queue[0]\n",
    "                        contig_queue = np.delete(contig_queue, 0)\n",
    "                    else:\n",
    "                        seq[sequence_pointer:sequence_pointer+gap_queue[0]] = '-'\n",
    "                        sequence_pointer += gap_queue[0]\n",
    "                        gap_queue = np.delete(gap_queue, 0)\n",
    "\n",
    "                    # We alternate between contigs and gaps\n",
    "                    iscontig = not iscontig\n",
    "\n",
    "            return sequences\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        def predict_sequence(self, seq, predict_only_gaps):\n",
    "\n",
    "            scaffold = self.preprocess_sequences([seq])\n",
    "            pred = self.autoencoder.predict(scaffold).reshape(self.max_seq_length, len(self.classes))\n",
    "\n",
    "            # Set the probability of empty \"-\" to zero, since we always want to predict something\n",
    "            emptyclass = np.where(self.ohe.transform(np.array(\"-\").reshape(-1, 1))[0])[0][0]\n",
    "            pred[:, emptyclass] = 0.0\n",
    "\n",
    "            # Convert the probability distribution to a one-hot encoded vector\n",
    "            mask = pred == np.amax(pred, axis=1).reshape(pred.shape[0], 1)\n",
    "            indices = list((i, np.where(mask[i])[0][0]) for i in range(mask.shape[0]))\n",
    "\n",
    "            pred = np.zeros(pred.shape)\n",
    "            for i in indices:\n",
    "                pred[i] = 1\n",
    "\n",
    "            # Then we can use our one hot encoder to convert back to the original sequence of classes\n",
    "            pred = self.ohe.inverse_transform(pred[:len(seq), :len(self.classes)]).reshape(len(seq))\n",
    "\n",
    "            if predict_only_gaps:\n",
    "                # We only care about predicting the gaps in seq, so replace amino acids in prediction with original nongaps\n",
    "                nongaps = np.where(seq != '-')[0]\n",
    "                pred[nongaps] = seq[nongaps]\n",
    "\n",
    "            return pred\n",
    "\n",
    "        # =============================================================================\n",
    "        def preprocess_sequences(self, seqs: List[np.array]) -> np.array:\n",
    "\n",
    "            # the value -1 lets numpy know to infer the shape. So it's just a column vector of length num_samples\n",
    "            seqs = [np.array(seq).reshape(-1, 1) for seq in seqs]\n",
    "\n",
    "            # One-hot encode each sequence\n",
    "            seqs = [self.ohe.fit_transform(seq) for seq in seqs]\n",
    "\n",
    "            if self.paddingtype == PaddingType.ZERO:\n",
    "                # The sequences may have different lengths, so we will pad them with zeros\n",
    "                # We are padding to fill up to max length, then we can turn it into a single numpy tensor\n",
    "                return np.array([np.pad(seq, ((0, self.max_seq_length-len(seq)), (0, 0))) for seq in seqs])\n",
    "\n",
    "            elif self.paddingtype == PaddingType.EMPTY:\n",
    "\n",
    "                # We pad with the \"empty\" class\n",
    "                emptyclass = self.ohe.fit_transform(np.array(\"-\").reshape(-1, 1))[0]\n",
    "\n",
    "                # dynamically extend each seq by enough emptyvals to make a single sequence length\n",
    "                return np.array([np.vstack((seq, *(emptyclass for _ in range(self.max_seq_length-len(seq))))) for seq in seqs])\n",
    "\n",
    "            elif self.paddingtype == PaddingType.TRUNCATE:\n",
    "                # FIXME: Implement me\n",
    "                raise Exception(\"Have not implemented Padding Type TRUNCATE!\")\n",
    "                \n",
    "         # =============================================================================\n",
    "        def train(self, verbose=\"auto\", optimizer=\"adam\", epochs=None):\n",
    "            self.optimizer = optimizer\n",
    "            if epochs:\n",
    "                self.epochs = epochs\n",
    "\n",
    "            self.autoencoder.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "            callbacks = []\n",
    "            if self.early_stopping:\n",
    "                callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10))\n",
    "            self.history = self.autoencoder.fit(self.train_x, self.train_y, epochs=self.epochs, validation_split=0.15, verbose=verbose, callbacks=callbacks)\n",
    "            \n",
    "            \n",
    "    ut.mkdir_if_not_exists(RESULTS_PATH)\n",
    "    ut.mkdir_if_not_exists(DATAPATH)\n",
    "\n",
    "    epochs = 200\n",
    "    noise_percent = 0.25\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # change to the location of the data for reading\n",
    "    os.chdir(DATAPATH)\n",
    "    de_novo_sequence = get_sequences(\"de_novo_sequence.txt\")[0]\n",
    "    target_sequence = get_sequences(\"target_sequence.txt\")[0]\n",
    "    alltrainingdata = get_sequences(\"training_sequences.txt\")\n",
    "    metadata = \"training_sequences_metadata.csv\"\n",
    "\n",
    "    # then change to the results directory for writing\n",
    "    os.chdir(RESULTS_PATH)\n",
    "\n",
    "    sequences_to_train_on = [1000]\n",
    "    # sequences_to_train_on = [100, 500, 1000, 2000]\n",
    "\n",
    "    trainfilename = \"cda_breakdown_breakdown_of_numtrain.csv\"\n",
    "    headers = [\"Num Training Instances\", \"Full Accuracy\", \"Gap Accuracy\", \"Nongap Accuracy\"]\n",
    "    with open(trainfilename, \"w+\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        train_writer = csv.writer(f)\n",
    "        train_writer.writerow(headers)\n",
    "\n",
    "    train_file = open(trainfilename, \"a+\", encoding=\"utf-8\", newline=\"\")\n",
    "    train_writer = csv.writer(train_file)\n",
    "\n",
    "    for numtrain in sequences_to_train_on:\n",
    "\n",
    "        trainingdata = alltrainingdata[:numtrain]\n",
    "        random.shuffle(trainingdata)\n",
    "\n",
    "        incorrect_indices = np.where(de_novo_sequence != target_sequence)[0]\n",
    "        correct_indices = np.where(de_novo_sequence == target_sequence)[0]\n",
    "\n",
    "        fixer = ProteinScaffoldFixer(trainingdata, noise_percent=noise_percent, epochs=epochs)\n",
    "        fixer.autoencoder.compile(optimizer=fixer.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        print(type(fixer.autoencoder))\n",
    "    return fixer.autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9fe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.KT_hp_build.<locals>.Autoencoder'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 12:56:53.312266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 12:56:55.061815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14759 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:19:00.0, compute capability: 7.5\n",
      "2023-06-08 12:56:55.062501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14759 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2023-06-08 12:56:55.062998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14759 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2023-06-08 12:56:55.063462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14008 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.KT_hp_build.<locals>.Autoencoder at 0x7fd185c28820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "KT_hp_build(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f348e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle  \n",
    "  \n",
    "# # Open a file and use dump()\n",
    "# with open('train_x.pkl', 'wb') as file:\n",
    "      \n",
    "#     # A new file will be created\n",
    "#     pickle.dump(train_x, file)\n",
    "\n",
    "#     # Open a file and use dump()\n",
    "# with open('train_y.pkl', 'wb') as file:\n",
    "      \n",
    "#     # A new file will be created\n",
    "#     pickle.dump(train_y, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a3b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 440, 24)\n",
      "(1000, 440, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "  \n",
    "# Open the file in binary mode\n",
    "with open('train_x.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    train_x = pickle.load(file)\n",
    "\n",
    "    # Open the file in binary mode\n",
    "with open('train_y.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    train_y = pickle.load(file)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14257b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6eb8528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.KT_hp_build.<locals>.Autoencoder'>\n"
     ]
    }
   ],
   "source": [
    "#initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel= KT_hp_build,\n",
    "    objective=\"val_accuracy\", \n",
    "    overwrite = True,# Do not resume the previous search in the same directory.\n",
    "    max_trials= 15,\n",
    "    directory = \"models/param_tuning\",  # Set a directory to store the intermediate results.\n",
    "    project_name= \"param_tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcfe173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "conv1_filter_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 1, 'sampling': 'linear'}\n",
      "conv2_filter_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 1, 'sampling': 'linear'}\n",
      "conv1_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 30, 'max_value': 50, 'step': 4, 'sampling': 'linear'}\n",
      "conv2_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 100, 'step': 4, 'sampling': 'linear'}\n",
      "bridge_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 120, 'max_value': 200, 'step': 8, 'sampling': 'linear'}\n",
      "bridge_filter_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 1, 'sampling': 'linear'}\n",
      "max_pool (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "dropout_:  (Choice)\n",
      "{'default': 0.25, 'conditions': [], 'values': [0.25, 0.35, 0.4, 0.5], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bed7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 01m 20s]\n",
      "val_accuracy: 0.97404545545578\n",
      "\n",
      "Best val_accuracy So Far: 0.9742878675460815\n",
      "Total elapsed time: 00h 19m 48s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#BATCH_SIZE = 64\n",
    "MAX_EPOCH_LENGTH = 200\n",
    "tuner.search(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=MAX_EPOCH_LENGTH,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    validation_split=0.15,\n",
    "    callbacks = [(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c719da4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in models/param_tuning/param_tuning\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7fd185aabd00>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 5\n",
      "conv2_filter_size: 9\n",
      "conv1_filters: 46\n",
      "conv2_filters: 90\n",
      "bridge_filters: 160\n",
      "bridge_filter_size: 5\n",
      "max_pool: 2\n",
      "dropout_: : 0.5\n",
      "Score: 0.9742878675460815\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 10\n",
      "conv2_filter_size: 10\n",
      "conv1_filters: 50\n",
      "conv2_filters: 74\n",
      "bridge_filters: 192\n",
      "bridge_filter_size: 5\n",
      "max_pool: 2\n",
      "dropout_: : 0.5\n",
      "Score: 0.974060595035553\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 4\n",
      "conv2_filter_size: 8\n",
      "conv1_filters: 38\n",
      "conv2_filters: 94\n",
      "bridge_filters: 120\n",
      "bridge_filter_size: 5\n",
      "max_pool: 4\n",
      "dropout_: : 0.25\n",
      "Score: 0.97404545545578\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 4\n",
      "conv2_filter_size: 5\n",
      "conv1_filters: 46\n",
      "conv2_filters: 78\n",
      "bridge_filters: 168\n",
      "bridge_filter_size: 5\n",
      "max_pool: 3\n",
      "dropout_: : 0.35\n",
      "Score: 0.9734696745872498\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 4\n",
      "conv2_filter_size: 8\n",
      "conv1_filters: 38\n",
      "conv2_filters: 94\n",
      "bridge_filters: 144\n",
      "bridge_filter_size: 9\n",
      "max_pool: 4\n",
      "dropout_: : 0.35\n",
      "Score: 0.9721817970275879\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 10\n",
      "conv2_filter_size: 6\n",
      "conv1_filters: 38\n",
      "conv2_filters: 86\n",
      "bridge_filters: 144\n",
      "bridge_filter_size: 6\n",
      "max_pool: 3\n",
      "dropout_: : 0.35\n",
      "Score: 0.9721212387084961\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 7\n",
      "conv2_filter_size: 5\n",
      "conv1_filters: 42\n",
      "conv2_filters: 78\n",
      "bridge_filters: 144\n",
      "bridge_filter_size: 5\n",
      "max_pool: 4\n",
      "dropout_: : 0.5\n",
      "Score: 0.97198486328125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 4\n",
      "conv2_filter_size: 6\n",
      "conv1_filters: 46\n",
      "conv2_filters: 66\n",
      "bridge_filters: 152\n",
      "bridge_filter_size: 10\n",
      "max_pool: 3\n",
      "dropout_: : 0.25\n",
      "Score: 0.9706515073776245\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 9\n",
      "conv2_filter_size: 5\n",
      "conv1_filters: 42\n",
      "conv2_filters: 58\n",
      "bridge_filters: 144\n",
      "bridge_filter_size: 8\n",
      "max_pool: 3\n",
      "dropout_: : 0.25\n",
      "Score: 0.9703636169433594\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv1_filter_size: 5\n",
      "conv2_filter_size: 9\n",
      "conv1_filters: 50\n",
      "conv2_filters: 50\n",
      "bridge_filters: 200\n",
      "bridge_filter_size: 7\n",
      "max_pool: 2\n",
      "dropout_: : 0.4\n",
      "Score: 0.9701363444328308\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66665c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
